{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from time import time\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from time import time\n",
    "from os import path\n",
    "from nltk import SklearnClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from fasttext import train_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2dict(string):\n",
    "    string = string.replace(\"{\" ,\"\")\n",
    "    string = string.replace(\"}\" , \"\")\n",
    "    dictionary = {}\n",
    "    for i in string.split(\", \"):\n",
    "        keyvalue = i.split(\":\")\n",
    "        dictionary[''.join(keyvalue[:-1]).strip(' \\'')] = int(keyvalue[-1])\n",
    "    return dictionary    \n",
    "\n",
    "\n",
    "# CONVERTS count of distinct words TO persent of distinct words in text:\n",
    "#   {'sad': 5, 'komornik': 15} -> {'sad': 0.25, 'komornik': 0.75}\n",
    "def cowd2powd(dictionary):  \n",
    "    d = dictionary.copy()             \n",
    "    count_of_words = sum(d.values())\n",
    "    for key, value in d.items():\n",
    "        d[key] = value/count_of_words*100\n",
    "    return d\n",
    "\n",
    "\n",
    "def get_clfs(clf_names):\n",
    "    clfs = []\n",
    "    for clf_name in  clf_names:\n",
    "        if clf_name == 'KNN':\n",
    "            clfs.append(SklearnClassifier(KNeighborsClassifier(n_jobs=-1)))\n",
    "        elif clf_name == 'RaF':\n",
    "            clfs.append(SklearnClassifier(RandomForestClassifier(n_jobs=-1)))\n",
    "        elif clf_name == 'LoR':\n",
    "            clfs.append(SklearnClassifier(LogisticRegression(n_jobs=-1)))\n",
    "        elif clf_name == 'MLP':\n",
    "            clfs.append(SklearnClassifier(MLPClassifier()))\n",
    "    return clfs\n",
    "\n",
    "\n",
    "def get_probs(class_probs, class_types):\n",
    "    confidence = []\n",
    "    for typ in class_types:\n",
    "        confidence.append(class_probs.prob(typ))\n",
    "    return np.array(confidence)\n",
    "\n",
    "\n",
    "def get_final_class(preds, id_typ_documents):\n",
    "    final_class = -1\n",
    "    votes = []\n",
    "    for p in preds:\n",
    "        votes.append(p.max())\n",
    "    \n",
    "    if votes.count(votes[0]) == len(votes):\n",
    "        final_class = votes[0]\n",
    "    else:\n",
    "        probs = np.zeros(len(id_typ_documents))\n",
    "        for p in preds: \n",
    "            probs += get_probs(p, id_typ_documents)\n",
    "        probs = list(probs)\n",
    "        final_class = id_typ_documents[probs.index(max(probs))]\n",
    "    return final_class\n",
    "\n",
    "\n",
    "def test_sklearn_clf(train_fpath, test_fpath, clfs, is_podw=False, type_cname='type'):\n",
    "    fsc, ttime = [], []\n",
    "    y_pred_probs = []\n",
    "\n",
    "    df_train = pd.read_csv(train_fpath, sep=';')\n",
    "    df_test = pd.read_csv(test_fpath, sep=';')\n",
    "\n",
    "    df_train['x'] = df_train['codw'].apply(lambda x: str2dict(x))\n",
    "    df_test['x'] = df_test['codw'].apply(lambda x: str2dict(x))\n",
    "    \n",
    "    if is_podw:\n",
    "        df_train['x'] = df_train['x'].apply(lambda x: cowd2powd(x))\n",
    "        df_test['x'] = df_test['x'].apply(lambda x: cowd2powd(x))\n",
    "\n",
    "    train_val = df_train[['x', type_cname]].values.tolist()\n",
    "\n",
    "    for clf in clfs:\n",
    "        start_time = time()\n",
    "        clf.train(train_val)\n",
    "        ttime.append(time() - start_time)\n",
    "\n",
    "        y_pred_probs.append(clf.prob_classify_many(df_test['x'].tolist()))\n",
    "        y_pred = clf.classify_many(df_test['x'].tolist())\n",
    "        \n",
    "        fsc.append(f1_score(df_test[type_cname].tolist(), y_pred, average='macro'))\n",
    "\n",
    "    id_typ_documents = sorted(list(set(df_train[type_cname])))\n",
    "\n",
    "    y_pred_probs_concat = []\n",
    "    for preds in np.array(y_pred_probs).T:\n",
    "        y_pred_probs_concat.append(get_final_class(preds, id_typ_documents))\n",
    "    \n",
    "    fsc.append(f1_score(df_test[type_cname].tolist(), y_pred_probs_concat, average='macro'))\n",
    "    ttime.append(sum(ttime))\n",
    "\n",
    "    return fsc, ttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/limit_5K_per_type_order_by_id_desc'\n",
    "samples = ['cleaned', 'stemmed', 'lemmatized']\n",
    "\n",
    "codw_folder = 'codw'\n",
    "split_num = 5\n",
    "\n",
    "# clf_names = ['KNN', 'RaF', 'LoR', 'MLP']\n",
    "clf_names = ['KNN', 'RaF', 'LoR']\n",
    "\n",
    "columns = ['sample'] + [clf_name + '_fsc' for clf_name in clf_names] + ['MULTI_fsc'] + [clf_name + '_ttime' for clf_name in clf_names] + ['MULTI_ttime']\n",
    "\n",
    "if not path.exists(f'{data_path}/20_results'):\n",
    "    os.mkdir(f'{data_path}/20_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2d21eb0b584c6885f8ef2431f2c2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "samples:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4847aafa2d624310b08f3c111fb3aa3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb30d8452e746fca40c93c3e6b0a1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c255427f7244a99f97e772d3f9de43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for sample in tqdm_notebook(samples, desc='samples'):\n",
    "    row = {'sample' : sample}\n",
    "\n",
    "    fscs, ttimes = [], []\n",
    "\n",
    "    for i in tqdm_notebook(range(split_num), desc='folds'):\n",
    "        train_fpath = f\"{data_path}/10_tt_split/{sample}/{codw_folder}/train_{i}.csv\"\n",
    "        test_fpath = f\"{data_path}/10_tt_split/{sample}/{codw_folder}/test_{i}.csv\"\n",
    "\n",
    "        fsc, ttime = test_sklearn_clf(train_fpath, test_fpath, get_clfs(clf_names))\n",
    "        fscs.append(fsc)\n",
    "        ttimes.append(ttime)\n",
    "\n",
    "    fscs = np.array(fscs).T\n",
    "    ttimes = np.array(ttimes).T\n",
    "\n",
    "    for e, clf_name in enumerate(clf_names):\n",
    "        row[f'{clf_name}_fsc'] = sum(fscs[e])/split_num\n",
    "        row[f'{clf_name}_ttime'] = sum(ttimes[e])/split_num\n",
    "\n",
    "    row['MULTI_fsc'] = sum(fscs[-1])/split_num\n",
    "    row['MULTI_ttime'] = sum(ttimes[-1])/split_num\n",
    "\n",
    "    df = df.append(row, ignore_index=True)\n",
    "\n",
    "    df.to_csv(f'{data_path}/20_results/codw.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PODW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5034d9f68e4b4596210fe7a7181631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "samples:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b464a5eef74580a3b3aae6cf98c70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184742da9bd8499a8855ec6b6d9682ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727bf500ff484da29bbf0df00a4a849d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for sample in tqdm_notebook(samples, desc='samples'):\n",
    "    row = {'sample' : sample}\n",
    "\n",
    "    fscs, ttimes = [], []\n",
    "\n",
    "    for i in tqdm_notebook(range(split_num), desc='folds'):\n",
    "        train_fpath = f\"{data_path}/10_tt_split/{sample}/{codw_folder}/train_{i}.csv\"\n",
    "        test_fpath = f\"{data_path}/10_tt_split/{sample}/{codw_folder}/test_{i}.csv\"\n",
    "\n",
    "        fsc, ttime = test_sklearn_clf(train_fpath, test_fpath, get_clfs(clf_names), is_podw=True)\n",
    "        fscs.append(fsc)\n",
    "        ttimes.append(ttime)\n",
    "\n",
    "    fscs = np.array(fscs).T\n",
    "    ttimes = np.array(ttimes).T\n",
    "\n",
    "    for e, clf_name in enumerate(clf_names):\n",
    "        row[f'{clf_name}_fsc'] = sum(fscs[e])/split_num\n",
    "        row[f'{clf_name}_ttime'] = sum(ttimes[e])/split_num\n",
    "\n",
    "    row['MULTI_fsc'] = sum(fscs[-1])/split_num\n",
    "    row['MULTI_ttime'] = sum(ttimes[-1])/split_num\n",
    "\n",
    "    df = df.append(row, ignore_index=True)\n",
    "\n",
    "    df.to_csv(f'{data_path}/20_results/podw.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_results_fasttext(params, path2sample, split_num, label):\n",
    "    list_of_fsc, list_of_ttime = [], []\n",
    "\n",
    "    for i in range(split_num):\n",
    "        train_file_path = f\"{path2sample}/train_{i}.csv\"\n",
    "        test_file_path = f\"{path2sample}/test_{i}.csv\"\n",
    "\n",
    "        start_time = time()\n",
    "        ft_model = train_supervised(input=train_file_path, \n",
    "                                    label=label, \n",
    "                                    epoch=params['epoch'],\n",
    "                                    lr=params['lr'],\n",
    "                                    loss=params['loss'],\n",
    "                                    minCount=params['minCount'],\n",
    "                                    wordNgrams=params['wordNgrams'],\n",
    "                                    ws=params['ws'],\n",
    "                                    dim=params['dim'],\n",
    "                                    neg=params['neg'])\n",
    "        list_of_ttime.append(time() - start_time)\n",
    "\n",
    "        a, prec, rec = ft_model.test(test_file_path)\n",
    "\n",
    "        list_of_fsc.append(2*prec*rec/(prec+rec))\n",
    "    \n",
    "    return np.mean(np.array(list_of_fsc), axis=0), np.mean(np.array(list_of_ttime)), np.std(np.array(list_of_fsc), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/limit_5K_per_type_order_by_id_desc'\n",
    "samples = ['cleaned', 'stemmed', 'lemmatized']\n",
    "\n",
    "split_num = 5\n",
    "fasttext_folder = 'fasttext'\n",
    "label = '__class__'\n",
    "\n",
    "epoch = 10\n",
    "lr = 3\n",
    "loss = 'ova'\n",
    "wordNgrams = 3\n",
    "\n",
    "minCount = 1\n",
    "dim = 50\n",
    "ws = 3\n",
    "neg = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a08274c4cc340dc97251c65f0b31468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3862df7422574285a25444c857c96ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lr:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fff51e2c794037a3c83521e98fc431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a95f2c758654db990162bc12e437ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7734f9c99b4ab7ae338bfc13639995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97edd306ca04046b74bab4f629dbd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lr:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0010e5b8b434fe59226eb153ee111d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008f55e83c914bdcb72db9936d66149e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010507bf853341e49095adbf718d35fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdf36c265884c8f806d8111585e6a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "lr:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c08a4c993494b3e9cd3d01161023d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c955b708b343e6b0fe38d5713ef0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3e2647dab9454b8edd6b4868a74b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loss:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params2test = { 'epoch': [5,10,20],\n",
    "                'lr': [1,2,3],\n",
    "                'loss': ['softmax', 'ova']}\n",
    "\n",
    "# df = pd.DataFrame(columns=[key for key in params2test.keys()] + [f'{sample}_fsc' for sample in samples] + [f'{sample}_ttime' for sample in samples])\n",
    "df = pd.read_csv(f'{data_path}/20_results/fasttext.csv', sep=';')\n",
    "\n",
    "for epoch in tqdm_notebook(params2test['epoch'], desc='epoch'):\n",
    "    for lr in tqdm_notebook(params2test['lr'], desc='lr'):\n",
    "        for loss in tqdm_notebook(params2test['loss'], desc='loss'):\n",
    "\n",
    "            row = {'epoch': epoch, 'lr': lr, 'loss': loss, 'wordNgrams': wordNgrams, 'minCount': minCount, 'dim': dim, 'ws': ws, 'neg': neg}\n",
    "            for sample in samples:\n",
    "                fsc, ttime, fsc_std = get_avg_results_fasttext(row, f\"{data_path}/10_tt_split/{sample}/{fasttext_folder}\", split_num, label)\n",
    "\n",
    "                row[f'{sample}_fsc'] = fsc\n",
    "                row[f'{sample}_fsc_std'] = fsc_std\n",
    "                row[f'{sample}_ttime'] = ttime\n",
    "            \n",
    "            df = df.append(row, ignore_index=True)\n",
    "            df.to_csv(f'{data_path}/20_results/fasttext.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ft2pd(path):\n",
    "    df = pd.read_csv(path, names=['text'], sep=';')\n",
    "    df['type'] = df['text'].apply(lambda text: int(text.split()[0].replace('__class__', '')))\n",
    "    df['text'] = df['text'].apply(lambda text: text.split()[1:])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame()\n",
    "\n",
    "clfs = {'KNN': KNeighborsClassifier(n_jobs=-1), \n",
    "        'RaF': RandomForestClassifier(n_jobs=-1), \n",
    "        'LoR': LogisticRegression(n_jobs=-1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242407e0fafe4cde9ced76b759132eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "samples:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eea34c580eb42f8b6fbb5bba3480fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4b5091d8c24bd2b927c9f47dc04f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb4e2faa5c44bc8abcca3796e330e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for sample in tqdm_notebook(samples, desc='samples'):\n",
    "    row = {'sample': sample, 'vector_size': 300, 'epoch': 100, 'dm': 0}\n",
    "\n",
    "    # df = pd.read_csv(f'{data_path}/10_tt_split/{sample}.csv', sep=';')\n",
    "    # df['text'] = df['text'].apply(lambda x: x.split())\n",
    "    # train_tagged = df.apply(lambda r: TaggedDocument(words=r['text'], tags=[r['type']]), axis=1).tolist()\n",
    "\n",
    "    # start_time = time()\n",
    "    # model = Doc2Vec(train_tagged, vector_size=row['vector_size'], epoch=row['epoch'], dm=row['dm'], workers=multiprocessing.cpu_count())\n",
    "    # row['d2v_ttime'] = (time()-start_time)/60\n",
    "\n",
    "    d2v_ttime, d2v_ctime, fsc, ttime = [], [], [], []\n",
    "    for i in tqdm_notebook(range(split_num), desc='split'):\n",
    "        df_train = format_ft2pd(f'{data_path}/10_tt_split/{sample}/{fasttext_folder}/train_{i}.csv')\n",
    "        df_test = format_ft2pd(f'{data_path}/10_tt_split/{sample}/{fasttext_folder}/test_{i}.csv')\n",
    "\n",
    "        start_time = time()\n",
    "        train_tagged = df_train.apply(lambda r: TaggedDocument(words=r['text'], tags=[r['type']]), axis=1).tolist()\n",
    "        model = Doc2Vec(train_tagged, vector_size=row['vector_size'], epoch=row['epoch'], dm=row['dm'], workers=multiprocessing.cpu_count())\n",
    "        \n",
    "        df_train['vec'] = df_train['text'].apply(lambda text: model.infer_vector(text))\n",
    "        df_test['vec'] = df_test['text'].apply(lambda text: model.infer_vector(text))\n",
    "        d2v_ttime.append(time()-start_time)\n",
    "        \n",
    "        fsc_split, ttime_split = [], []\n",
    "        for clf in clfs.values():\n",
    "            start_time = time()\n",
    "            clf.fit(df_train['vec'].tolist(), df_train['type'].tolist())\n",
    "            ttime_split.append(time()-start_time)\n",
    "            y_pred = clf.predict(df_test['vec'].tolist())\n",
    "            fsc_split.append(f1_score(df_test['type'].tolist(), y_pred, average='macro'))\n",
    "\n",
    "        ttime.append(ttime_split)\n",
    "        fsc.append(fsc_split)\n",
    "\n",
    "    d2v_ttime = np.mean(np.array(d2v_ttime))\n",
    "    fsc_std = np.std(np.array(fsc).T, axis=1)\n",
    "    fsc = np.mean(np.array(fsc).T, axis=1)\n",
    "    ttime = np.mean(np.array(ttime).T, axis=1)\n",
    "\n",
    "    for key, f, f_std, tt in zip(clfs.keys(), fsc, fsc_std, ttime):\n",
    "        row[f'{key}_ttime'] = (tt + d2v_ttime)/60\n",
    "        row[f'{key}_fsc'] = f\n",
    "        row[f'{key}_fsc_std'] = f_std\n",
    "\n",
    "    df_res = df_res.append(row, ignore_index=True)\n",
    "    df_res.to_csv(f'{data_path}/20_results/doc2vec.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "135a5e2811e807938e4c3a7d694011e30bcf72392809a1353e1d4a87fa9f6710"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
