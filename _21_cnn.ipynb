{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw59ZnAZQRwr",
        "outputId": "54756616-b3e4-450c-accc-8d5fc50a4ad5"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'keras.backend' has no attribute '_get_available_gpus'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\YAHORY~1\\AppData\\Local\\Temp/ipykernel_14436/515872547.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_available_gpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute '_get_available_gpus'"
          ]
        }
      ],
      "source": [
        "from keras import backend as K\n",
        "# K._get_available_gpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lsPjk5W4QmMp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import keras\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os, re, csv, math, codecs\n",
        "\n",
        "\n",
        "MAX_NB_WORDS = 1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# path = 'drive/MyDrive/Colab Notebooks/data/cleaned.csv'\n",
        "# ft_path = 'drive/MyDrive/Colab Notebooks/fasttext/cc.pl.300.vec'\n",
        "ft_path = 'data/fasttext/cc.pl.300.vec'\n",
        "path = 'data/limit_5K_per_type_order_by_id_desc/10_tt_split/cleaned.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT3I1ysjQtrp",
        "outputId": "f087cfb3-9acd-4e1e-c76d-595513d8af73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenizing input data...\n",
            "dictionary size:  542531\n"
          ]
        }
      ],
      "source": [
        "df_concat = pd.read_csv(path, sep=';')\n",
        "df_concat['doc_len'] = df_concat['text'].apply(lambda words: len(words.split(\" \")))\n",
        "max_seq_len = np.round(df_concat['doc_len'].mean() + df_concat['doc_len'].std()).astype(int)\n",
        "\n",
        "raw_docs = df_concat['text'].tolist()\n",
        "label_names = list(set(df_concat['type']))\n",
        "num_classes = len(label_names)\n",
        "\n",
        "print(\"tokenizing input data...\")\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(raw_docs)  #leaky\n",
        "word_seq = tokenizer.texts_to_sequences(raw_docs)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"dictionary size: \", len(word_index))\n",
        "\n",
        "#pad sequences\n",
        "word_seq = sequence.pad_sequences(word_seq, maxlen=max_seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYbwuGFxVPv0"
      },
      "source": [
        "# LOAD FASTTEXT EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFRpY1OVVNwF",
        "outputId": "dbffc9a2-fc5b-4d35-e6e1-a2bd8721e41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading word embeddings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2000001it [03:31, 9450.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "found 2000000 word vectors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#load embeddings\n",
        "print('loading word embeddings...')\n",
        "embeddings_index = {}\n",
        "f = codecs.open(ft_path, encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('found %s word vectors' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BSdezwAVRdU",
        "outputId": "4ba79843-2a40-410c-bc7f-0e70a7effb7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 454368\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 300 \n",
        "\n",
        "#embedding matrix\n",
        "print('preparing embedding matrix...')\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edd4YRfJVapE",
        "outputId": "9cf60139-d72d-4c86-9354-aff8ed45a723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample words not found:  ['rtł' 'pomnaizgawąęfaę' 'watpliwnśei' 'gbpnr' 'geąsoz' 'ośvirbdaam'\n",
            " 'komentka' 'pożżęi' 'nalrżżsżżci' 'zarqu']\n"
          ]
        }
      ],
      "source": [
        "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vPnV3jjjVhdU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from operator import itemgetter \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import tensorflow as tf \n",
        "from time import time\n",
        "\n",
        "\n",
        "def lable2vec(lable, label_names):\n",
        "    res = np.zeros(len(label_names))\n",
        "    res[label_names.index(lable)] = 1\n",
        "    return res\n",
        "\n",
        "  \n",
        "def train_model(x_train, y_train, nb_words, embed_dim, params):\n",
        "  #CNN architecture\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(nb_words, embed_dim,\n",
        "            weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
        "  model.add(Conv1D(params['num_filters'][0], params['cernel_size'], activation='relu', padding='same'))\n",
        "  model.add(MaxPooling1D(2))\n",
        "  model.add(Conv1D(params['num_filters'][1], params['cernel_size'], activation='relu', padding='same'))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  model.add(Dropout(params['dropout']))\n",
        "  model.add(Dense(params['dence_nnum'], activation='relu', kernel_regularizer=regularizers.l2(params['weight_decay'])))\n",
        "  model.add(Dense(len(y_train[0]), activation='softmax'))  \n",
        "\n",
        "  adam = tf.optimizers.Adam(params['lrate'])\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "  # model.summary()\n",
        "\n",
        "  # define callbacks\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "  callbacks_list = [early_stopping]\n",
        "  hist = model.fit(x_train, y_train, batch_size=params['batch_size'], epochs=params['epochs'], callbacks=callbacks_list, validation_split=params['validation_split'], shuffle=True, verbose=2)\n",
        "\n",
        "  # hist = model.fit(x_train, y_train, batch_size=params['batch_size'], epochs=params['epochs'], validation_split=params['validation_split'], shuffle=True, verbose=2)\n",
        "  \n",
        "  return model, hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cEYzixc6Byx9"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(df_concat, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train = itemgetter(train.index.tolist())(word_seq)\n",
        "x_test = itemgetter(test.index.tolist())(word_seq)\n",
        "\n",
        "y_train = np.array([lable2vec(lable, label_names) for lable in train['type'].tolist()])\n",
        "y_test = np.array(test['type'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h1qPlvcPBS6m"
      },
      "outputs": [],
      "source": [
        "# train_model(x_train, y_train, nb_words, embed_dim, 10, 0.0001, 128, 0.5, [300, 128], 9, 64, 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Dmhbhx2UD8k0"
      },
      "outputs": [],
      "source": [
        "# df_res = pd.DataFrame()\n",
        "# df_res = pd.read_csv('drive/MyDrive/Colab Notebooks/data/w2v_cnn.csv', sep=';')\n",
        "df_res = pd.read_csv('data/limit_5K_per_type_order_by_id_desc/20_results/w2v_cnn.csv', sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "lrate = 0.001\n",
        "batch_size = 64\n",
        "dropout = 0.2\n",
        "num_filters = [256, 128]\n",
        "cernel_size = 3\n",
        "dence_nnum = 64\n",
        "weight_decay = 0.0001\n",
        "\n",
        "validation_split = 0.2\n",
        "\n",
        "# for dropout in [0.1, 0.3, 0.5, 0.7]:\n",
        "# for num_filters in [[64, 64], [128, 64], [128, 128], [256, 128]]:\n",
        "# for cernel_size in [3, 6, 9, 12]:\n",
        "for dence_nnum in [32, 64, 128, 256]:\n",
        "  print('dence_nnum:', dence_nnum)\n",
        "\n",
        "  row = {\n",
        "      'sample': 'cleaned',\n",
        "      'epochs': epochs,\n",
        "      'lrate': lrate,\n",
        "      'batch_size': batch_size,\n",
        "      'dropout': dropout,\n",
        "      'num_filters': num_filters,\n",
        "      'cernel_size': cernel_size,\n",
        "      'dence_nnum': dence_nnum,\n",
        "      'weight_decay': weight_decay,\n",
        "      'validation_split': validation_split\n",
        "  }\n",
        "\n",
        "  start = time()\n",
        "  model, hist = train_model(x_train, y_train, nb_words, embed_dim, row)\n",
        "\n",
        "  row['ttime'] = (time() - start)/60\n",
        "  row['val_loss'] = hist.history['val_loss'][-1]\n",
        "  row['val_accuracy'] = hist.history['val_accuracy'][-1]\n",
        "\n",
        "  df_res = df_res.append(row, ignore_index=True)\n",
        "  df_res.to_csv('drive/MyDrive/Colab Notebooks/data/w2v_cnn.csv', index=False, sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srOGgQ-R9u3X"
      },
      "outputs": [],
      "source": [
        "df_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg2PQVX3YELx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TalpaSoft_document_classification_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
