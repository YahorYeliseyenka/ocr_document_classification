{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from os import path\n",
    "from stempel import StempelStemmer\n",
    "from lib.preprocessing import lower_text\n",
    "from lib.preprocessing import remove_numbers_punctuation_whitespaces\n",
    "from lib.preprocessing import remove_stopwords\n",
    "from lib.preprocessing import apply_stempel_stemmer\n",
    "from lib.preprocessing import apply_spacy_lemmatize\n",
    "from lib.preprocessing import remove_empty_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "with open('data/stopwords', 'r', encoding='UTF-8') as f:\n",
    "    STOPWORDS = [line.replace('\\n','') for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_length = 2\n",
    "min_count_of_words = 10\n",
    "\n",
    "dset_path = 'data/limit_5K_per_type_order_by_id_desc'\n",
    "\n",
    "df_path = f'{dset_path}/00_chosen_types/chosen_types_df.csv'\n",
    "\n",
    "df_clean_path = f'{dset_path}/01_processed/cleaned.csv'\n",
    "df_stemm_path = f'{dset_path}/01_processed/stemmed.csv'\n",
    "df_lemma_path = f'{dset_path}/01_processed/lemmatized.csv'\n",
    "\n",
    "if not path.exists(f'{dset_path}/01_processed'):\n",
    "    os.mkdir(f'{dset_path}/01_processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaving id, doc_type and text cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_final_names = {'id_dokument': 'id', 'text': 'text', 'id_typ_dokument_true': 'type'}\n",
    "\n",
    "df = pd.read_csv(df_path, sep=';')\n",
    "\n",
    "df = df[c_final_names.keys()]\n",
    "df = df.rename(columns=c_final_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274966/274966 [00:10<00:00, 26835.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serching 4 all words with length > 2 && removing numbers, punctuation and whitespaces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274966/274966 [02:42<00:00, 1693.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274966/274966 [21:58<00:00, 208.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr to str...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 274966/274966 [01:28<00:00, 3121.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ENDED in 28.230589834849038min\n",
      "Removing documents having fewer then 10 words... \n",
      "\tBEFORE : count of documents = 274966\n",
      "\tAFTER : count of documents = 271885\n",
      "Writing to data/limit_5K_per_type_order_by_id_desc/01_processed/cleaned.csv...\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "print('Lowering...')\n",
    "df['text'] = df['text'].progress_apply(lower_text)\n",
    "\n",
    "print(f'Serching 4 all words with length > {min_word_length} && removing numbers, punctuation and whitespaces...')\n",
    "df['text'] = df['text'].progress_apply(remove_numbers_punctuation_whitespaces, args=[min_word_length])\n",
    "\n",
    "print('Removing stopwords...')\n",
    "df['text'] = df['text'].progress_apply(remove_stopwords, args=[STOPWORDS])\n",
    "\n",
    "print('Arr to str...')\n",
    "df['text'] = df['text'].progress_apply(lambda words: ' '.join(words))\n",
    "\n",
    "print(f'Process ENDED in {(time()-start_time)/60}min')\n",
    "\n",
    "print(f'Removing documents having fewer then {min_count_of_words} words...',\n",
    "            f\"\\n\\tBEFORE : count of documents = {df.shape[0]}\")\n",
    "df = remove_empty_documents(df, min_count_of_words)\n",
    "print(f\"\\tAFTER : count of documents = {df.shape[0]}\")\n",
    "\n",
    "print(f'Writing to {df_clean_path}...')\n",
    "df.to_csv(df_clean_path, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning using stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from data/limit_5K_per_type_order_by_id_desc/01_processed/cleaned.csv...\n",
      "Loading stemmer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████| 11368252/11368252 [00:09<00:00, 1202604.44bytes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [1:01:04<00:00, 74.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr to str...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [01:46<00:00, 2542.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 4 all words with length > 2 && removing numbers, punctuation and whitespaces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [01:11<00:00, 3827.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [16:35<00:00, 273.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr to str...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [01:18<00:00, 3466.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ENDED in 85.49818791945775min\n",
      "Writing to data/limit_5K_per_type_order_by_id_desc/01_processed/stemmed.csv...\n"
     ]
    }
   ],
   "source": [
    "print(f'Reading from {df_clean_path}...')\n",
    "df = pd.read_csv(df_clean_path, sep=';')\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "print('Loading stemmer...')\n",
    "stemmer = StempelStemmer.polimorf()\n",
    "\n",
    "print('Stemming...')\n",
    "if type(df['text'][0]) == str:\n",
    "    df['text'] = df['text'].apply(lambda text: str(text).split())\n",
    "df['text'] = df['text'].progress_apply(apply_stempel_stemmer, args=[stemmer, min_word_length])\n",
    "\n",
    "print('Arr to str...')\n",
    "df['text'] = df['text'].progress_apply(lambda words: ' '.join(words))\n",
    "\n",
    "print(f'Searching 4 all words with length > {min_word_length} && removing numbers, punctuation and whitespaces...')\n",
    "df['text'] = df['text'].progress_apply(remove_numbers_punctuation_whitespaces, args=[min_word_length])\n",
    "\n",
    "print('Removing stopwords...')\n",
    "df['text'] = df['text'].progress_apply(remove_stopwords, args=[STOPWORDS])\n",
    "\n",
    "print('Arr to str...')\n",
    "df['text'] = df['text'].progress_apply(lambda words: ' '.join(words))\n",
    "\n",
    "print(f'Process ENDED in {(time()-start_time)/60}min')\n",
    "\n",
    "print(f'Writing to {df_stemm_path}...')\n",
    "df.to_csv(df_stemm_path, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning using lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from data/limit_5K_per_type_order_by_id_desc/01_processed/cleaned.csv...\n",
      "Loading stemmer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████| 11368252/11368252 [00:09<00:00, 1182653.45bytes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lemmatizer...\n",
      "Lemmatizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [6:30:26<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr to str...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [02:00<00:00, 2247.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 4 all words with length > 2 && removing numbers, punctuation and whitespaces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [01:35<00:00, 2852.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [14:54<00:00, 304.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr to str...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271885/271885 [00:59<00:00, 4550.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ENDED in 412.9527033408483min\n",
      "Writing to data/limit_5K_per_type_order_by_id_desc/01_processed/lemmatized.csv...\n"
     ]
    }
   ],
   "source": [
    "print(f'Reading from {df_clean_path}...')\n",
    "df = pd.read_csv(df_clean_path, sep=';')\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "print('Loading stemmer...')\n",
    "stemmer = StempelStemmer.polimorf()\n",
    "\n",
    "print('Loading lemmatizer...')\n",
    "lemmatizer = spacy.load(\"pl_core_news_md\")\n",
    "\n",
    "print('Lemmatizing...')\n",
    "if type(df['text'][0]) != str:\n",
    "    df['text'] = df['text'].apply(lambda words: ' '.join(words))\n",
    "df['text'] = df['text'].progress_apply(apply_spacy_lemmatize, args=[lemmatizer])\n",
    "\n",
    "print('Arr to str...')\n",
    "df['text'] = df['text'].progress_apply(lambda words: ' '.join(words))\n",
    "\n",
    "print(f'Searching 4 all words with length > {min_word_length} && removing numbers, punctuation and whitespaces...')\n",
    "df['text'] = df['text'].progress_apply(remove_numbers_punctuation_whitespaces, args=[min_word_length])\n",
    "\n",
    "print('Removing stopwords...')\n",
    "df['text'] = df['text'].progress_apply(remove_stopwords, args=[STOPWORDS])\n",
    "\n",
    "print('Arr to str...')\n",
    "df['text'] = df['text'].progress_apply(lambda words: ' '.join(words))\n",
    "\n",
    "print(f'Process ENDED in {(time()-start_time)/60}min')\n",
    "\n",
    "print(f'Writing to {df_lemma_path}...')\n",
    "df.to_csv(df_lemma_path, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "135a5e2811e807938e4c3a7d694011e30bcf72392809a1353e1d4a87fa9f6710"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
